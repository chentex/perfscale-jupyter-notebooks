{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "- Install touchstone. get config file and import required modules"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip3 install git+https://github.com/Ayesha279/benchmark-comparison.git@multiRecord_touchstone\n",
    "!curl -LO https://raw.githubusercontent.com/Ayesha279/benchmark-comparison/multiRecord_touchstone/config/ocm-requests.json\n",
    "    \n",
    "from touchstone.benchmarks.generic import Benchmark\n",
    "from touchstone import decision_maker\n",
    "from touchstone import databases\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Set touchstone paramters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "uuid=\"aeed6306-b7e1-11eb-b313-e86a640406b2\"\n",
    "uuid2=\"1bf34d07-b3ef-4f57-a612-234c3f2d4ba1\"\n",
    "database=\"elasticsearch\"\n",
    "es_index = \"ocm-requests\"\n",
    "es_url=os.environ.get('ES_URL')\n",
    "benchmark=Benchmark(open(\"ocm-requests.json\"), database)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Connect to database - ElasticSearch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "database_instance = databases.grab(database, conn_url = es_url)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Iterate the indexes from the config file and get timeseries results from each one of them"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "document_list = []  \n",
    "for compute in benchmark.compute_map['ocm-requests'] :\n",
    "    print(\"Getting results\")\n",
    "    timeseries_result1 = database_instance.get_timeseries_results(uuid=uuid, \n",
    "                                                                 compute_map = compute,\n",
    "                                                                 index = es_index,\n",
    "                                                                 identifier=\"uuid\"\n",
    "                                                                )\n",
    "    \n",
    "df = pd.DataFrame(timeseries_result1)\n",
    "\n",
    "for compute in benchmark.compute_map['ocm-requests'] :\n",
    "    print(\"Getting results\")\n",
    "    timeseries_result2 = database_instance.get_timeseries_results(uuid=uuid2, \n",
    "                                                                 compute_map = compute,\n",
    "                                                                 index = es_index,\n",
    "                                                                 identifier=\"uuid\"\n",
    "                                                                )\n",
    "dff = pd.DataFrame(timeseries_result2)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Plot results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, axs = plt.subplots(2,figsize=(15, 10))\n",
    "plt.title('Latency comparision')\n",
    "plt.ylabel('Latency (ns)')\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df.groupby(df[\"timestamp\"].dt.second)[\"latency_ns\"].mean().plot(rot=0, ax=axs[0])\n",
    "\n",
    "plt.ylabel('Latency (nc)')\n",
    "dff[\"timestamp\"] = pd.to_datetime(dff[\"timestamp\"])\n",
    "dff.groupby(dff[\"timestamp\"].dt.second)[\"latency_ns\"].mean().plot(rot=0, ax=axs[1])\n",
    "\n",
    "\n",
    "axs[0].title.set_text('UUID- aeed6306-b7e1-11eb-b313-e86a640406b2')\n",
    "axs[1].title.set_text('UUID- 1bf34d07-b3ef-4f57-a612-234c3f2d4ba1')\n",
    "fig.suptitle('Latency comparision', fontsize=16)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "Test1=df[\"latency_ns\"].mean()\n",
    "Test2=dff['latency_ns'].mean()\n",
    "\n",
    "plotdata = pd.DataFrame(\n",
    "    {\"Latency_ns\": [Test1, Test2]}, \n",
    "    index=[\"Test1\", \"Test2\"])\n",
    "\n",
    "plotdata.plot(kind=\"bar\")\n",
    "plt.title('Avrage Latency Comparision')\n",
    "plt.ylabel('Latency (ns)')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}